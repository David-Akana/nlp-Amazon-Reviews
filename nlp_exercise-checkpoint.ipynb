{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be working with product review text from Amazon. The reviews are for only products in the \"Electronics\" category. The objective is to train a model to predict the rating, ranging from 1 to 5 stars, based on the customer review, as well as get whatever extra insight I can from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open(\"Desktop/Stuff/amazon_electronics_reviews_training.json.gz\", \"r\") as f:                                  \n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings are stored in the keyword \"overall\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ratings = np.array([rev['overall'] for rev in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create a transformer that extracts the corpus from the 'reviewText' keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return [rev['reviewText'] for rev in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, ratings, test_size=0.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('transformer', MyTransformer()), ('vectorizer', TfidfVectorizer(stop_words=STOP_WORDS)),\n",
    "                ('regressor', Ridge())])\n",
    "\n",
    "param_grid = {'regressor__alpha': np.logspace(-3, 2, 20)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, param_grid, cv=5, n_jobs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('transformer', MyTransformer()), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "       fit_params=None, iid='warn', n_jobs=2,\n",
       "       param_grid={'regressor__alpha': array([1.00000e-03, 1.83298e-03, 3.35982e-03, 6.15848e-03, 1.12884e-02,\n",
       "       2.06914e-02, 3.79269e-02, 6.95193e-02, 1.27427e-01, 2.33572e-01,\n",
       "       4.28133e-01, 7.84760e-01, 1.43845e+00, 2.63665e+00, 4.83293e+00,\n",
       "       8.85867e+00, 1.62378e+01, 2.97635e+01, 5.45559e+01, 1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__alpha': 1.438449888287663}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('transformer', MyTransformer()), ('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll use X_test and y_test to train my model since it's a lot bigger than X_train and y_train\n",
    "\n",
    "pipe.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43865741905267175"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To derive some insight from my analysis, I want to determine the most polarizing words in the corpus of reviews. In other words, identify words that strongly signal a review is either positive or negative. For example, a word like \"terrible\" will mostly appear in negative rather than positive reviews. The naive Bayes model calculates probabilities such as  ùëÉ(terrible | negative) , the probability the review is negative given the word \"terrible\" appears in the text. Using these probabilities, I can derive a polarity score for each counted word,\n",
    "\n",
    "polarity = log(ùëÉ(word | positive) / ùëÉ(word | negative))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dataset I'll be using only has reviews rated at 1 and 5 stars\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "with gzip.open(\"Desktop/Stuff/amazon_one_and_five_star_reviews.json.gz\", \"r\") as f:\n",
    "    data_polarity = [json.loads(line) for line in f]\n",
    "\n",
    "ratings = np.array([rev['overall'] for rev in data_polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_model = MyTransformer()\n",
    "tr = tr_model.fit_transform(data_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "tr_t = vec.fit_transform(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(tr_t, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The naive bayes model has an attribute that handles the positive and negative logarithmic probability feature for each word\n",
    "\n",
    "log_array = nb_model.feature_log_prob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref variable will contain all the words in the corpus\n",
    "\n",
    "ref = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure I have the same number of words\n",
    "\n",
    "assert log_array.shape[1] == len(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making a dictionary with each word as the key and the log of their positive and negaive polarities attached to each key\n",
    "# To find the negative polarity, I subtract the log of the probability of the word being negative by the log of the\n",
    "# probability of the word being positive, and vise versa for the positive polarity\n",
    "\n",
    "mapping = {word: [] for word in ref}\n",
    "\n",
    "for i in range(len(ref)):\n",
    "    mapping[ref[i]].append(log_array[0, i]-log_array[1, i])\n",
    "    mapping[ref[i]].append(log_array[1, i]-log_array[0, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting to extract the words with the highest polarities\n",
    "\n",
    "negative_polarities = sorted([log_array[0, i]-log_array[1, i] for i in range(len(ref))])[-25:]\n",
    "positive_polarities = sorted([log_array[1, i]-log_array[0, i] for i in range(len(ref))])[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50 = []\n",
    "\n",
    "for polarity in negative_polarities:\n",
    "    for value in ref:\n",
    "        if mapping[value][0] == polarity:\n",
    "            top_50.append(value)\n",
    "            \n",
    "for polarity in positive_polarities:\n",
    "    for value in ref:\n",
    "        if mapping[value][1] == polarity:\n",
    "            top_50.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['refused',\n",
       " 'threw',\n",
       " 'disappointing',\n",
       " 'randomly',\n",
       " 'stopped',\n",
       " 'unreliable',\n",
       " 'horrible',\n",
       " 'awful',\n",
       " 'unacceptable',\n",
       " 'poor',\n",
       " 'beware',\n",
       " 'defective',\n",
       " 'trash',\n",
       " 'worse',\n",
       " 'worthless',\n",
       " 'useless',\n",
       " 'garbage',\n",
       " 'returned',\n",
       " 'terrible',\n",
       " 'junk',\n",
       " 'worst',\n",
       " 'returning',\n",
       " 'return',\n",
       " 'waste',\n",
       " 'refund',\n",
       " 'regrets',\n",
       " 'fantastic',\n",
       " 'buck',\n",
       " 'telephoto',\n",
       " 'photography',\n",
       " 'crisp',\n",
       " 'dslr',\n",
       " 'portraits',\n",
       " 'awesome',\n",
       " 'handy',\n",
       " 'charm',\n",
       " '200mm',\n",
       " 'pleased',\n",
       " 'bokeh',\n",
       " 'excellent',\n",
       " 'incredible',\n",
       " 'macro',\n",
       " 'sturdy',\n",
       " 'amazing',\n",
       " 'portrait',\n",
       " 'monopod',\n",
       " 'perfect',\n",
       " 'protects',\n",
       " 'beat',\n",
       " 'highly']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From this, I was able to get the top 50 polarizing words that often indicate a positive or negative rating\n",
    "\n",
    "top_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll apply Topic Modelling using the Non-Negative Matrix Factorization model to try to extract topics that exist within the reviews to further get more information from my data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "really little tv battery need case does ve bought don power time cd work player radio unit like dvd use \n",
      "\n",
      "Topic: 1\n",
      "pictures photography image low macro kit fast 70 wide shots cap zoom nikon hood 50mm light sharp lenses focus canon \n",
      "\n",
      "Topic: 2\n",
      "listening like don headphone set hear better great quality noise head volume ears comfortable music pair bass ear speakers headphones \n",
      "\n",
      "Topic: 3\n",
      "signal computer printer need extension video connectors long audio monitor say monster belkin quality length connect usb tv needed cables \n",
      "\n",
      "Topic: 4\n",
      "shots carry picture case zoom small memory photos batteries strap tripod use cameras card battery flash canon digital pictures bag \n",
      "\n",
      "Topic: 5\n",
      "need use worked highly exactly expected just buy bought item advertised easy perfectly needed say recommend fine price product works \n",
      "\n",
      "Topic: 6\n",
      "haze polarizing clean glare multi job coated expensive polarizer lens quality lenses does hoya protection protect tiffen glass uv filters \n",
      "\n",
      "Topic: 7\n",
      "ve keys computer hand laptop button microsoft using used scroll ball mice wheel optical use buttons trackball logitech usb keyboard \n",
      "\n",
      "Topic: 8\n",
      "set worked problems install port pc modem laptop support setup internet windows usb netgear connection computer network linksys card wireless \n",
      "\n",
      "Topic: 9\n",
      "cheap better really build say high does cables wire nice sound pretty excellent job value recommend buy product quality price \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Well, not much can be taken from the first topic, but from the second it definitely involves photography using a nikon camera,\n",
    "# the third about the sound generated from headphones and the forth probably a desktop system\n",
    "\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "nmf = NMF(n_components=n_topics, random_state=0)\n",
    "pipe = Pipeline([('tr', MyTransformer()), ('vectorizer', tfidf), ('dim-red', nmf)])\n",
    "\n",
    "pipe.fit(data)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "\n",
    "for i, topic in enumerate(nmf.components_):\n",
    "    print(\"Topic: {}\".format(i))\n",
    "    indices = topic.argsort()[-n_top_words-1:-1]\n",
    "    top_words = [feature_names[ind] for ind in indices]\n",
    "    print(\" \".join(top_words), \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
